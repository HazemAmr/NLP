{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C6_9t23obhb",
        "outputId": "c56fe8f6-8499-4faf-f3db-a2e52cba72f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6llGFf3oh3Z",
        "outputId": "65e136dd-d257-43e8-f34c-453797d000be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gpt-2-simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmZ8_irzoWoJ",
        "outputId": "a38f3087-1b08-46cc-bad6-7da3f9807250"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.25.2)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0GgsIBboALc",
        "outputId": "5d34ab20-eb0a-45f1-86df-30799eb4400b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 3.94Git/s]                                                     \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.69Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 3.99Git/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:08, 60.8Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 1.41Git/s]                                               \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 7.54Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 8.19Mit/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "Loading pretrained model models/124M/model.ckpt\n",
            "Loading pretrained model models/124M/model.ckpt\n",
            "Generated Document 1:\n",
            "Lionel Messi was the legend of the summer. Was he the most brilliant player in existence? Or did he suffer from an underlying mental breakdown?\n",
            "\n",
            "The answer is a resounding 'no'. The only person who has ever been a great footballer for more than 20 years is Lionel Messi.\n",
            "\n",
            "The world's 10 best players\n",
            "\n",
            "1) Neymar\n",
            "\n",
            "Neymar has been in my personal best XI for years. For a long time, he was the best player in the world.\n",
            "\n",
            "\n",
            "Generated Document 2:\n",
            "Cristiano Ronaldo: A sad man, but a great man.\n",
            "\n",
            "\"I want to be a coach, not just because of my role, but also because I want to win trophies.\n",
            "\n",
            "\"I am excited for the season but I need to do a better job of it.\n",
            "\n",
            "\"If I am not the best player in the world, I will lose people I love, but I have no problem with winning games, winning titles.\"<|endoftext|>Embed This Video On Your Site With This H\n",
            "\n",
            "Generated Document 3:\n",
            "Neymar JR: We have been asked to provide information about the situation in Saigon and about an ongoing operation to capture the fugitive. We are willing to provide that information to them if they wish to do so.\"\n",
            "\n",
            "The company said it was aware of the fire but that it could not provide information about the conditions on Saigon's outskirts and eventually, could not release the details.\n",
            "\n",
            "\"We are not making any guarantees that our investigation will be successful,\" said a spokesman for the company. \"We\n",
            "\n",
            "Document 1:\n",
            "Word: year, TF-IDF: 0.27\n",
            "Word: xi, TF-IDF: 0.13\n",
            "Word: world, TF-IDF: 0.20\n",
            "Word: wa, TF-IDF: 0.30\n",
            "Word: underlying, TF-IDF: 0.13\n",
            "Word: time, TF-IDF: 0.13\n",
            "Word: summer, TF-IDF: 0.13\n",
            "Word: suffer, TF-IDF: 0.13\n",
            "Word: resounding, TF-IDF: 0.13\n",
            "Word: player, TF-IDF: 0.30\n",
            "Word: personal, TF-IDF: 0.13\n",
            "Word: person, TF-IDF: 0.13\n",
            "Word: no, TF-IDF: 0.13\n",
            "Word: neymar, TF-IDF: 0.20\n",
            "Word: messi, TF-IDF: 0.27\n",
            "Word: mental, TF-IDF: 0.13\n",
            "Word: long, TF-IDF: 0.13\n",
            "Word: lionel, TF-IDF: 0.27\n",
            "Word: legend, TF-IDF: 0.13\n",
            "Word: ha, TF-IDF: 0.27\n",
            "Word: great, TF-IDF: 0.10\n",
            "Word: footballer, TF-IDF: 0.13\n",
            "Word: existence, TF-IDF: 0.13\n",
            "Word: ever, TF-IDF: 0.13\n",
            "Word: brilliant, TF-IDF: 0.13\n",
            "Word: breakdown, TF-IDF: 0.13\n",
            "Word: best, TF-IDF: 0.30\n",
            "Word: answer, TF-IDF: 0.13\n",
            "Word: 20, TF-IDF: 0.13\n",
            "Word: 10, TF-IDF: 0.13\n",
            "\n",
            "Document 2:\n",
            "Word: world, TF-IDF: 0.12\n",
            "Word: winning, TF-IDF: 0.33\n",
            "Word: win, TF-IDF: 0.16\n",
            "Word: want, TF-IDF: 0.33\n",
            "Word: video, TF-IDF: 0.16\n",
            "Word: trophy, TF-IDF: 0.16\n",
            "Word: title, TF-IDF: 0.16\n",
            "Word: site, TF-IDF: 0.16\n",
            "Word: season, TF-IDF: 0.16\n",
            "Word: sad, TF-IDF: 0.16\n",
            "Word: ronaldo, TF-IDF: 0.16\n",
            "Word: role, TF-IDF: 0.16\n",
            "Word: problem, TF-IDF: 0.16\n",
            "Word: player, TF-IDF: 0.12\n",
            "Word: people, TF-IDF: 0.16\n",
            "Word: need, TF-IDF: 0.16\n",
            "Word: man, TF-IDF: 0.33\n",
            "Word: love, TF-IDF: 0.16\n",
            "Word: lose, TF-IDF: 0.16\n",
            "Word: job, TF-IDF: 0.16\n",
            "Word: great, TF-IDF: 0.12\n",
            "Word: game, TF-IDF: 0.16\n",
            "Word: excited, TF-IDF: 0.16\n",
            "Word: endoftext, TF-IDF: 0.16\n",
            "Word: embed, TF-IDF: 0.16\n",
            "Word: cristiano, TF-IDF: 0.16\n",
            "Word: coach, TF-IDF: 0.16\n",
            "Word: better, TF-IDF: 0.16\n",
            "Word: best, TF-IDF: 0.12\n",
            "Word: also, TF-IDF: 0.16\n",
            "\n",
            "Document 3:\n",
            "Word: wish, TF-IDF: 0.13\n",
            "Word: willing, TF-IDF: 0.13\n",
            "Word: wa, TF-IDF: 0.10\n",
            "Word: successful, TF-IDF: 0.13\n",
            "Word: spokesman, TF-IDF: 0.13\n",
            "Word: situation, TF-IDF: 0.13\n",
            "Word: saigon, TF-IDF: 0.27\n",
            "Word: said, TF-IDF: 0.27\n",
            "Word: release, TF-IDF: 0.13\n",
            "Word: provide, TF-IDF: 0.40\n",
            "Word: outskirt, TF-IDF: 0.13\n",
            "Word: operation, TF-IDF: 0.13\n",
            "Word: ongoing, TF-IDF: 0.13\n",
            "Word: neymar, TF-IDF: 0.10\n",
            "Word: making, TF-IDF: 0.13\n",
            "Word: jr, TF-IDF: 0.13\n",
            "Word: investigation, TF-IDF: 0.13\n",
            "Word: information, TF-IDF: 0.40\n",
            "Word: guarantee, TF-IDF: 0.13\n",
            "Word: fugitive, TF-IDF: 0.13\n",
            "Word: fire, TF-IDF: 0.13\n",
            "Word: eventually, TF-IDF: 0.13\n",
            "Word: detail, TF-IDF: 0.13\n",
            "Word: could, TF-IDF: 0.27\n",
            "Word: condition, TF-IDF: 0.13\n",
            "Word: company, TF-IDF: 0.27\n",
            "Word: capture, TF-IDF: 0.13\n",
            "Word: aware, TF-IDF: 0.13\n",
            "Word: asked, TF-IDF: 0.13\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# Function to download GPT-2 model files\n",
        "def download_gpt2_model():\n",
        "    gpt2.download_gpt2(model_name=\"124M\")\n",
        "\n",
        "# Function to generate text using GPT-2\n",
        "def generate_text(prompt, model_name=\"124M\", length=100):\n",
        "    # Reset TensorFlow graph to clear existing variables\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    # Start a new TensorFlow session\n",
        "    sess = gpt2.start_tf_sess()\n",
        "\n",
        "    # Load the pre-trained model checkpoint (if not already loaded)\n",
        "    gpt2.load_gpt2(sess, model_name=model_name)\n",
        "\n",
        "    # Generate text based on the prompt\n",
        "    generated_doc = gpt2.generate(sess, model_name=model_name, prefix=prompt, length=length, return_as_list=True)[0]\n",
        "\n",
        "    # Close the TensorFlow session\n",
        "    gpt2.reset_session(sess)\n",
        "\n",
        "    return generated_doc\n",
        "\n",
        "# Define three different topic prompts\n",
        "topic_prompt_1 = \"Lionel Messi\"\n",
        "topic_prompt_2 = \"Cristiano Ronaldo\"\n",
        "topic_prompt_3 = \"Neymar JR\"\n",
        "\n",
        "# Download GPT-2 model files\n",
        "download_gpt2_model()\n",
        "\n",
        "# Generate documents for each topic\n",
        "generated_document_1 = generate_text(topic_prompt_1)\n",
        "generated_document_2 = generate_text(topic_prompt_2)\n",
        "generated_document_3 = generate_text(topic_prompt_3)\n",
        "\n",
        "# Print generated documents\n",
        "print(\"Generated Document 1:\")\n",
        "print(generated_document_1)\n",
        "print()\n",
        "\n",
        "print(\"Generated Document 2:\")\n",
        "print(generated_document_2)\n",
        "print()\n",
        "\n",
        "print(\"Generated Document 3:\")\n",
        "print(generated_document_3)\n",
        "print()\n",
        "\n",
        "\n",
        "# Step 1: Text preprocessing for generated documents\n",
        "def preprocess_text(documents):\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    preprocessed_documents = []\n",
        "    for doc in documents:\n",
        "        # Convert to lowercase\n",
        "        doc = doc.lower()\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(doc)\n",
        "        # Lemmatization\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "        # Remove stop words\n",
        "        tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "        preprocessed_documents.append(\" \".join(tokens))\n",
        "    return preprocessed_documents\n",
        "\n",
        "preprocessed_documents = preprocess_text([generated_document_1, generated_document_2, generated_document_3])\n",
        "\n",
        "# Step 2: TF-IDF calculation\n",
        "# Count Vectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "# Fit and transform the preprocessed documents\n",
        "count_matrix = count_vectorizer.fit_transform(preprocessed_documents)\n",
        "\n",
        "# TF-IDF Transformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "# Fit and transform the count matrix\n",
        "tfidf_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
        "\n",
        "# Get feature names (words)\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print TF-IDF scores for each document\n",
        "for i, doc in enumerate(preprocessed_documents):\n",
        "    print(f\"Document {i + 1}:\")\n",
        "    feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
        "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
        "        print(f\"Word: {w}, TF-IDF: {s:.2f}\")\n",
        "    print()\n"
      ]
    }
  ]
}